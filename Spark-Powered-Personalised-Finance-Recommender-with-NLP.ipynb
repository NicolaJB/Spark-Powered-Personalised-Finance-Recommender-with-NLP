{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Spark-Powered Personalised Finance Recommender with NLP**\n",
        "This demo showcases an end-to-end scalable pipeline that combines Spark’s big data processing capabilities with advanced NLP techniques to analyse customer sentiment from financial product reviews. Leveraging collaborative filtering and sentiment analysis, it delivers personalised product recommendations that enhance customer engagement and business insights for financial services."
      ],
      "metadata": {
        "id": "VtR18cH1NM04"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Environment Setup (PySpark in Colab):**"
      ],
      "metadata": {
        "id": "1we-hQEKIZTB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "l0Kj178PIWpH"
      },
      "outputs": [],
      "source": [
        "# Install Spark\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.4.3/spark-3.4.3-bin-hadoop3.tgz\n",
        "!tar -xzf spark-3.4.3-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.3-bin-hadoop3\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"FinanceRecommender\").getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Prepare Dummy Data:**"
      ],
      "metadata": {
        "id": "SGyt9YHSIgmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "data = [\n",
        "    Row(userId=1, itemId=101, rating=5.0, text=\"Low fees and great rewards program.\"),\n",
        "    Row(userId=1, itemId=102, rating=3.0, text=\"Good interest rates but limited customer service.\"),\n",
        "    Row(userId=2, itemId=101, rating=1.0, text=\"Hidden charges and poor online interface.\"),\n",
        "    Row(userId=2, itemId=103, rating=4.0, text=\"Reliable service and responsive customer support.\"),\n",
        "    Row(userId=3, itemId=104, rating=5.0, text=\"Excellent mobile app experience and fast transfers.\"),\n",
        "    Row(userId=3, itemId=105, rating=2.0, text=\"High minimum balance and confusing terms.\"),\n",
        "    Row(userId=4, itemId=102, rating=4.0, text=\"Helpful staff and competitive loan rates.\"),\n",
        "    Row(userId=4, itemId=106, rating=1.0, text=\"App crashes frequently and support is unhelpful.\"),\n",
        "    Row(userId=5, itemId=101, rating=3.0, text=\"Decent account features, but not innovative.\"),\n",
        "    Row(userId=5, itemId=107, rating=5.0, text=\"Outstanding investment tools and analytics.\"),\n",
        "    Row(userId=6, itemId=104, rating=2.0, text=\"Slow customer service and outdated interface.\"),\n",
        "    Row(userId=6, itemId=108, rating=4.0, text=\"Good savings plan and clear statements.\"),\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkxOCnxPIjYM",
        "outputId": "918da69f-f21f-4619-e931-e28024dddee1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+------+--------------------+\n",
            "|userId|itemId|rating|                text|\n",
            "+------+------+------+--------------------+\n",
            "|     1|   101|   5.0|Low fees and grea...|\n",
            "|     1|   102|   3.0|Good interest rat...|\n",
            "|     2|   101|   1.0|Hidden charges an...|\n",
            "|     2|   103|   4.0|Reliable service ...|\n",
            "|     3|   104|   5.0|Excellent mobile ...|\n",
            "|     3|   105|   2.0|High minimum bala...|\n",
            "|     4|   102|   4.0|Helpful staff and...|\n",
            "|     4|   106|   1.0|App crashes frequ...|\n",
            "|     5|   101|   3.0|Decent account fe...|\n",
            "|     5|   107|   5.0|Outstanding inves...|\n",
            "|     6|   104|   2.0|Slow customer ser...|\n",
            "|     6|   108|   4.0|Good savings plan...|\n",
            "+------+------+------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Basic NLP Pipeline (Spark only):**\n",
        "\n",
        "Scalable NLP - Spark NLP pipeline that processes user reviews\n",
        "\n",
        "- Scalable: Uses distributed processing in Spark — not just small datasets in memory.\n",
        "- NLP: Handles tokenization, stopword removal, TF-IDF vectorization.\n",
        "- Pipeline Architecture: Prepares data for downstream ML tasks like classification or recommendation.\n",
        "\n",
        "*This code shows ability to design scalable NLP pipelines using distributed frameworks — a key requirement for roles involving big data and AI at enterprise scale.*\n",
        "\n",
        "Defines a Spark ML pipeline for scalable NLP, performing:\n",
        "- Tokenization\n",
        "- Stop word removal\n",
        "- TF-IDF vectorization\n",
        "- Logistic regression classification:\n",
        "\n",
        "Perfect for sentiment classification or categorizing user feedback, especially in high-volume financial platforms.\n",
        "\n",
        "*Potential next step: add Named Entity Recognition (NER), sentiment classification, or BERT embeddings using Spark NLP.*\n",
        "\n"
      ],
      "metadata": {
        "id": "BBJWFV7cInvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
        "tf = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\")\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "classifier = LogisticRegression(featuresCol=\"features\", labelCol=\"rating\")\n",
        "\n",
        "nlp_pipeline = Pipeline(stages=[tokenizer, remover, tf, idf, classifier])\n",
        "nlp_model = nlp_pipeline.fit(df)\n",
        "\n",
        "df_predicted = nlp_model.transform(df)\n",
        "df_predicted.select(\"userId\", \"itemId\", \"text\", \"prediction\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lr5H_ITsIqPx",
        "outputId": "14e2f7d5-a374-4db8-b2d3-e7f783f7c4c1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+--------------------+----------+\n",
            "|userId|itemId|                text|prediction|\n",
            "+------+------+--------------------+----------+\n",
            "|     1|   101|Low fees and grea...|       5.0|\n",
            "|     1|   102|Good interest rat...|       3.0|\n",
            "|     2|   101|Hidden charges an...|       1.0|\n",
            "|     2|   103|Reliable service ...|       4.0|\n",
            "|     3|   104|Excellent mobile ...|       5.0|\n",
            "|     3|   105|High minimum bala...|       2.0|\n",
            "|     4|   102|Helpful staff and...|       4.0|\n",
            "|     4|   106|App crashes frequ...|       1.0|\n",
            "|     5|   101|Decent account fe...|       3.0|\n",
            "|     5|   107|Outstanding inves...|       5.0|\n",
            "|     6|   104|Slow customer ser...|       2.0|\n",
            "|     6|   108|Good savings plan...|       4.0|\n",
            "+------+------+--------------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. ALS Collaborative Filtering:**\n",
        "\n",
        "- Collaborative Filtering – ALS-based personalized product recommendation\n",
        "- uses Spark MLlib's Alternating Least Squares (ALS) algorithm to generate personalized recommendations\n",
        "\n",
        "Personalized product/content recommendation:\n",
        "- Suggesting financial tools/products to users\n",
        "- Offering tailored investment content"
      ],
      "metadata": {
        "id": "JfWjGF3lIviM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "als = ALS(userCol=\"userId\", itemCol=\"itemId\", ratingCol=\"rating\", coldStartStrategy=\"drop\")\n",
        "als_model = als.fit(df)\n",
        "\n",
        "userRecs = als_model.recommendForAllUsers(2)\n",
        "userRecs.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-F-NyZ8Iv_u",
        "outputId": "9e229c6a-98f7-4a30-9a77-0072d79c8e20"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------------------------------------+\n",
            "|userId|recommendations                     |\n",
            "+------+------------------------------------+\n",
            "|1     |[{107, 5.12471}, {101, 4.826678}]   |\n",
            "|2     |[{103, 3.8860695}, {102, 1.4257795}]|\n",
            "|3     |[{104, 4.852529}, {108, 2.6690392}] |\n",
            "|4     |[{102, 3.8416784}, {101, 2.7848132}]|\n",
            "|5     |[{107, 4.899176}, {101, 2.9993615}] |\n",
            "|6     |[{108, 3.8856165}, {104, 2.0011468}]|\n",
            "+------+------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Model Serialisation (Optional in Colab):**\n",
        "\n",
        "- Big Data Readiness – Uses Spark pipelines + serialization\"\n",
        "- use Spark ML pipelines and serialize models"
      ],
      "metadata": {
        "id": "7E9P0r_SIxer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "als_model.write().overwrite().save(\"/content/als_model\")\n",
        "nlp_model.write().overwrite().save(\"/content/nlp_pipeline_model\")"
      ],
      "metadata": {
        "id": "HPwC9MfcI1cJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Wrap-up application relevance**"
      ],
      "metadata": {
        "id": "kGMCdlBrI2im"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Highlight                   | Example                                                     |\n",
        "| --------------------------- | ----------------------------------------------------------- |\n",
        "| **Scalable NLP**            | Spark NLP pipeline that processes user reviews              |\n",
        "| **Collaborative Filtering** | ALS-based personalized product recommendation               |\n",
        "| **Big Data Readiness**      | Uses Spark pipelines + serialization                        |\n",
        "| **Practical Application**   | Can recommend financial products, assess customer sentiment |"
      ],
      "metadata": {
        "id": "WJY5042LJfS-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For larger datasets, tap into several real-world and publicly available sources, especially for finance/product reviews and user interactions:\n",
        "\n",
        "- Financial product reviews from sites like:\n",
        "- Yelp/Factual (business reviews with some financial products)\n",
        "- Amazon Reviews (for financial books, software, devices)\n",
        "- Kaggle datasets: e.g., Amazon product reviews, Yelp reviews\n",
        "\n",
        "**Open financial datasets:**\n",
        "\n",
        "- Quandl or Alpha Vantage — for market and financial product data (may include user ratings/comments if integrated)\n",
        "- Financial news/commentary APIs — could be mined for sentiment analysis\n",
        "- Banking or financial app data (if you have access or through partnerships):\n",
        "- Real user feedback on loans, credit cards, investment products\n",
        "- Transactional data with implicit ratings or feedback\n",
        "- Synthetic or simulated data generation:\n",
        "- Generate large-scale simulated user-item interactions and review texts to test scalability\n",
        "\n",
        "Social media and forums:\n",
        "- Twitter, Reddit (finance-related subreddits), StockTwits — rich sources for sentiment and user opinion mining"
      ],
      "metadata": {
        "id": "vvcEm--UNtEw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Zip the model as an archive folder:**"
      ],
      "metadata": {
        "id": "bqaIXJHqtGvD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r als_model.zip /content/als_model\n",
        "!zip -r nlp_pipeline_model.zip /content/nlp_pipeline_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7NMf4ittLyD",
        "outputId": "6bd1d202-69f2-485a-c009-482a6e57d5ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/als_model/ (stored 0%)\n",
            "  adding: content/als_model/itemFactors/ (stored 0%)\n",
            "  adding: content/als_model/itemFactors/part-00003-a6519213-e28c-4d63-857d-db11adda1a64-c000.snappy.parquet (deflated 30%)\n",
            "  adding: content/als_model/itemFactors/.part-00002-a6519213-e28c-4d63-857d-db11adda1a64-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: content/als_model/itemFactors/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/als_model/itemFactors/.part-00003-a6519213-e28c-4d63-857d-db11adda1a64-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: content/als_model/itemFactors/part-00002-a6519213-e28c-4d63-857d-db11adda1a64-c000.snappy.parquet (deflated 30%)\n",
            "  adding: content/als_model/itemFactors/part-00000-a6519213-e28c-4d63-857d-db11adda1a64-c000.snappy.parquet (deflated 31%)\n",
            "  adding: content/als_model/itemFactors/.part-00001-a6519213-e28c-4d63-857d-db11adda1a64-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: content/als_model/itemFactors/.part-00000-a6519213-e28c-4d63-857d-db11adda1a64-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: content/als_model/itemFactors/part-00001-a6519213-e28c-4d63-857d-db11adda1a64-c000.snappy.parquet (deflated 30%)\n",
            "  adding: content/als_model/itemFactors/_SUCCESS (stored 0%)\n",
            "  adding: content/als_model/metadata/ (stored 0%)\n",
            "  adding: content/als_model/metadata/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/als_model/metadata/.part-00000.crc (stored 0%)\n",
            "  adding: content/als_model/metadata/_SUCCESS (stored 0%)\n",
            "  adding: content/als_model/metadata/part-00000 (deflated 40%)\n",
            "  adding: content/als_model/userFactors/ (stored 0%)\n",
            "  adding: content/als_model/userFactors/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/als_model/userFactors/part-00001-3857ec6b-cc32-48b6-9a4c-fe00c9651942-c000.snappy.parquet (deflated 30%)\n",
            "  adding: content/als_model/userFactors/part-00000-3857ec6b-cc32-48b6-9a4c-fe00c9651942-c000.snappy.parquet (deflated 31%)\n",
            "  adding: content/als_model/userFactors/.part-00001-3857ec6b-cc32-48b6-9a4c-fe00c9651942-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: content/als_model/userFactors/.part-00000-3857ec6b-cc32-48b6-9a4c-fe00c9651942-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: content/als_model/userFactors/_SUCCESS (stored 0%)\n",
            "  adding: content/als_model/userFactors/.part-00002-3857ec6b-cc32-48b6-9a4c-fe00c9651942-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: content/als_model/userFactors/part-00002-3857ec6b-cc32-48b6-9a4c-fe00c9651942-c000.snappy.parquet (deflated 31%)\n",
            "  adding: content/nlp_pipeline_model/ (stored 0%)\n",
            "  adding: content/nlp_pipeline_model/stages/ (stored 0%)\n",
            "  adding: content/nlp_pipeline_model/stages/2_HashingTF_4b3567996e23/ (stored 0%)\n",
            "  adding: content/nlp_pipeline_model/stages/2_HashingTF_4b3567996e23/metadata/ (stored 0%)\n",
            "  adding: content/nlp_pipeline_model/stages/2_HashingTF_4b3567996e23/metadata/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/nlp_pipeline_model/stages/2_HashingTF_4b3567996e23/metadata/.part-00000.crc (stored 0%)\n",
            "  adding: content/nlp_pipeline_model/stages/2_HashingTF_4b3567996e23/metadata/_SUCCESS (stored 0%)\n",
            "  adding: content/nlp_pipeline_model/stages/2_HashingTF_4b3567996e23/metadata/part-00000 (deflated 34%)\n",
            "  adding: content/nlp_pipeline_model/stages/0_Tokenizer_93ef02099429/ (stored 0%)\n",
            "  adding: content/nlp_pipeline_model/stages/0_Tokenizer_93ef02099429/metadata/ (stored 0%)\n",
            "  adding: content/nlp_pipeline_model/stages/0_Tokenizer_93ef02099429/metadata/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/nlp_pipeline_model/stages/0_Tokenizer_93ef02099429/metadata/.part-00000.crc (stored 0%)\n",
            "  adding: content/nlp_pipeline_model/stages/0_Tokenizer_93ef02099429/metadata/_SUCCESS (stored 0%)\n",
            "  adding: content/nlp_pipeline_model/stages/0_Tokenizer_93ef02099429/metadata/part-00000 (deflated 33%)\n",
            "  adding: content/nlp_pipeline_model/stages/4_LogisticRegression_62cb13f26090/ (stored 0%)\n",
            "  adding: content/nlp_pipeline_model/stages/4_LogisticRegression_62cb13f26090/metadata/ (stored 0%)\n",
            "  adding: content/nlp_pipeline_model/stages/4_LogisticRegression_62cb13f26090/metadata/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/nlp_pipeline_model/stages/4_LogisticRegression_62cb13f26090/metadata/.part-00000.crc (stored 0%)\n",
            "  adding: content/nlp_pipeline_model/stages/4_LogisticRegression_62cb13f26090/metadata/_SUCCESS (stored 0%)\n",
            "  adding: content/nlp_pipeline_model/stages/4_LogisticRegression_62cb13f26090/metadata/part-00000 (deflated 43%)\n",
            "  adding: content/nlp_pipeline_model/stages/4_LogisticRegression_62cb13f26090/data/ (stored 0%)\n",
            "  adding: content/nlp_pipeline_model/stages/4_LogisticRegression_62cb13f26090/data/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/nlp_pipeline_model/stages/4_LogisticRegression_62cb13f26090/data/part-00000-805ad983-9b8f-4e81-a1b4-412d721c29ef-c000.snappy.parquet (deflated 61%)\n",
            "  adding: content/nlp_pipeline_model/stages/4_LogisticRegression_62cb13f26090/data/.part-00000-805ad983-9b8f-4e81-a1b4-412d721c29ef-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: content/nlp_pipeline_model/stages/4_LogisticRegression_62cb13f26090/data/_SUCCESS (stored 0%)\n",
            "  adding: content/nlp_pipeline_model/stages/3_IDF_7886f40040cd/ (stored 0%)\n",
            "  adding: content/nlp_pipeline_model/stages/3_IDF_7886f40040cd/metadata/ (stored 0%)\n",
            "  adding: content/nlp_pipeline_model/stages/3_IDF_7886f40040cd/metadata/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/nlp_pipeline_model/stages/3_IDF_7886f40040cd/metadata/.part-00000.crc (stored 0%)\n",
            "  adding: content/nlp_pipeline_model/stages/3_IDF_7886f40040cd/metadata/_SUCCESS (stored 0%)\n",
            "  adding: content/nlp_pipeline_model/stages/3_IDF_7886f40040cd/metadata/part-00000 (deflated 32%)\n",
            "  adding: content/nlp_pipeline_model/stages/3_IDF_7886f40040cd/data/ (stored 0%)\n",
            "  adding: content/nlp_pipeline_model/stages/3_IDF_7886f40040cd/data/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/nlp_pipeline_model/stages/3_IDF_7886f40040cd/data/.part-00000-0bc4578c-5bc3-4e12-9010-2324f80cb7c1-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: content/nlp_pipeline_model/stages/3_IDF_7886f40040cd/data/part-00000-0bc4578c-5bc3-4e12-9010-2324f80cb7c1-c000.snappy.parquet (deflated 55%)\n",
            "  adding: content/nlp_pipeline_model/stages/3_IDF_7886f40040cd/data/_SUCCESS (stored 0%)\n",
            "  adding: content/nlp_pipeline_model/stages/1_StopWordsRemover_05c108b11f94/ (stored 0%)\n",
            "  adding: content/nlp_pipeline_model/stages/1_StopWordsRemover_05c108b11f94/metadata/ (stored 0%)\n",
            "  adding: content/nlp_pipeline_model/stages/1_StopWordsRemover_05c108b11f94/metadata/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/nlp_pipeline_model/stages/1_StopWordsRemover_05c108b11f94/metadata/.part-00000.crc (stored 0%)\n",
            "  adding: content/nlp_pipeline_model/stages/1_StopWordsRemover_05c108b11f94/metadata/_SUCCESS (stored 0%)\n",
            "  adding: content/nlp_pipeline_model/stages/1_StopWordsRemover_05c108b11f94/metadata/part-00000 (deflated 58%)\n",
            "  adding: content/nlp_pipeline_model/metadata/ (stored 0%)\n",
            "  adding: content/nlp_pipeline_model/metadata/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/nlp_pipeline_model/metadata/.part-00000.crc (stored 0%)\n",
            "  adding: content/nlp_pipeline_model/metadata/_SUCCESS (stored 0%)\n",
            "  adding: content/nlp_pipeline_model/metadata/part-00000 (deflated 23%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('als_model.zip')\n",
        "files.download('nlp_pipeline_model.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "RVVMK-xztOhw",
        "outputId": "ee51a539-97ba-49cc-b4d1-3147fd70fa6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_3e234386-a248-43c3-8254-8a1c32f3bd83\", \"als_model.zip\", 10846)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e07a9b77-0a06-4023-9e59-e38e0d6f2682\", \"nlp_pipeline_model.zip\", 18319)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load the model back to use it:**"
      ],
      "metadata": {
        "id": "2HdOEJHZtn_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.recommendation import ALSModel\n",
        "from pyspark.ml import PipelineModel\n",
        "\n",
        "# Load ALS model\n",
        "als_model_loaded = ALSModel.load(\"/content/als_model\")\n",
        "\n",
        "# Load NLP pipeline model\n",
        "nlp_model_loaded = PipelineModel.load(\"/content/nlp_pipeline_model\")"
      ],
      "metadata": {
        "id": "GqHZkvqotm--"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare some example input data -\n",
        "Create a small DataFrame similar to the training data to run predictions on:"
      ],
      "metadata": {
        "id": "dn8OT9_jtq-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "test_data = [\n",
        "    Row(userId=1, itemId=103, rating=0.0, text=\"Unreliable service and support is unhelpful.\"),\n",
        "    Row(userId=2, itemId=101, rating=0.0, text=\"Sophisticated investment tools and competitive loan rates.\"),\n",
        "]\n",
        "\n",
        "test_df = spark.createDataFrame(test_data)"
      ],
      "metadata": {
        "id": "q8rfC-Mttwqp"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify user/item IDs in test data vs training data:\n",
        "\n",
        "Note:\n",
        "- Cold start problem: By default, the ALS model cannot predict for user or item IDs that were not present in the training data. These unknown users/items get filtered out (especially if you used coldStartStrategy=\"drop\").\n",
        "\n",
        "- Input Data: If test_df contains user or item IDs not in the training set, predictions for those pairs will be missing because the model doesn’t know how to handle them."
      ],
      "metadata": {
        "id": "25NwuJoQvQ-k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(\"userId\").distinct().show()\n",
        "df.select(\"itemId\").distinct().show()\n",
        "\n",
        "test_df.select(\"userId\").distinct().show()\n",
        "test_df.select(\"itemId\").distinct().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1H8aCSNvJNI",
        "outputId": "0ad64d66-16bd-43d6-de40-2cc3c70aa3cf"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+\n",
            "|userId|\n",
            "+------+\n",
            "|     1|\n",
            "|     3|\n",
            "|     2|\n",
            "|     6|\n",
            "|     5|\n",
            "|     4|\n",
            "+------+\n",
            "\n",
            "+------+\n",
            "|itemId|\n",
            "+------+\n",
            "|   103|\n",
            "|   104|\n",
            "|   105|\n",
            "|   101|\n",
            "|   102|\n",
            "|   107|\n",
            "|   106|\n",
            "|   108|\n",
            "+------+\n",
            "\n",
            "+------+\n",
            "|userId|\n",
            "+------+\n",
            "|     1|\n",
            "|     2|\n",
            "+------+\n",
            "\n",
            "+------+\n",
            "|itemId|\n",
            "+------+\n",
            "|   103|\n",
            "|   101|\n",
            "+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run predictions:**\n",
        "\n",
        "For the ALS recommendation model (predict ratings or get recommendations):\n",
        "- Learns embeddings for users and items\n",
        "- Makes predictions based on user–item interactions only\n",
        "\n",
        "Output: User (ID) is likely to rate Item (ID) around level (prediction)"
      ],
      "metadata": {
        "id": "_JKuPyWPt3Sg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "als_predictions = als_model.transform(test_df)\n",
        "#als_predictions = als_model_loaded.transform(test_df)\n",
        "als_predictions.select(\"userId\", \"itemId\", \"prediction\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mP3rbw41t5pN",
        "outputId": "f77db5b0-9776-4c9b-c754-c2f63520036e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+----------+\n",
            "|userId|itemId|prediction|\n",
            "+------+------+----------+\n",
            "|     2|   101|  0.999046|\n",
            "|     1|   103| 1.4784565|\n",
            "+------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the NLP pipeline (predict sentiment or classification):\n",
        "- Learns from language content\n",
        "- Predicts sentiment, often used to augment or explain ratings\n",
        "\n",
        "Output: 5 could mean most the positive sentiment, and 0 means negative"
      ],
      "metadata": {
        "id": "1YV0C3wTuESs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_predictions = nlp_model.transform(test_df)\n",
        "# nlp_predictions = nlp_model_loaded.transform(test_df)\n",
        "nlp_predictions.select(\"userId\", \"itemId\", \"text\", \"prediction\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ovCKtpeuE5e",
        "outputId": "505c1abb-d2c0-44b5-f62b-caf5f2909d52"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+--------------------+----------+\n",
            "|userId|itemId|                text|prediction|\n",
            "+------+------+--------------------+----------+\n",
            "|     1|   103|Unreliable servic...|       1.0|\n",
            "|     2|   101|Sophisticated inv...|       4.0|\n",
            "+------+------+--------------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Outputs Explained:**\n",
        "\n",
        "ALS: prediction shows the estimated rating for user-item pairs, which you can interpret as a recommendation score.\n",
        "\n",
        "NLP pipeline: prediction shows the predicted class label (e.g., sentiment) for the input text."
      ],
      "metadata": {
        "id": "hUKhFIiPuMBT"
      }
    }
  ]
}
